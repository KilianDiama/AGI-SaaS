# plugins/plugin_fusion_multi_llm.py

import logging
from noyau_core import BasePlugin, Context, Meta
import httpx

logger = logging.getLogger("plugin.fusion_multi_llm")

OLLAMA_URL = "http://localhost:11434/api/generate"

class FusionMultiLLMPlugin(BasePlugin):
    meta = Meta(
        name="fusion_multi_llm",
        priority=110,
        version="1.1",
        author="Toi & GPT"
    )

    async def call_ollama(self, prompt: str, model: str) -> str:
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        try:
            async with httpx.AsyncClient() as client:
                res = await client.post(OLLAMA_URL, json=payload)
                res.raise_for_status()
                return res.json().get("response", "")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Erreur avec le mod√®le {model} via Ollama : {e}")
            return f"[Erreur avec {model}]"

    async def run(self, ctx: Context) -> Context:
        prompt = ctx.get("llm_prompt") or ctx.get("payload", {}).get("message", "")
        models = ctx.get("llm_models", [])
        backend = ctx.get("llm_backend", "ollama")

        if not prompt or not models:
            logger.warning("‚ùå Pas de prompt ou de mod√®les sp√©cifi√©s.")
            ctx["response"] = "Aucune r√©ponse possible (pas de prompt ou de mod√®les)."
            return ctx

        if backend != "ollama":
            logger.warning("üö´ Backend non support√© (seul 'ollama' est g√©r√© ici).")
            ctx["response"] = "Ce plugin ne g√®re que les backends locaux (ollama)."
            return ctx

        responses = []
        for model in models:
            logger.info(f"ü§ñ Appel de {model} via Ollama...")
            reply = await self.call_ollama(prompt, model)
            responses.append(f"[{model}] {reply.strip()}")

        ctx["llm_responses"] = responses
        fusion = "\n\n".join([f"üîπ {r}" for r in responses])
        ctx["response_llm"] = f"ü§ñ **Fusion des mod√®les ({len(models)})** :\n\n{fusion}"

        ctx.setdefault("plugins_log", []).append(
            f"FusionMultiLLMPlugin : {len(models)} r√©ponses fusionn√©es."
        )
        return ctx
