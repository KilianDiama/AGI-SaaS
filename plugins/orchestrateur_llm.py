# plugins/orchestrateur_llm.py

from noyau_core import BasePlugin, Context, Meta
import logging

logger = logging.getLogger("plugin.orchestrateur_llm")

class OrchestrateurLLMPlugin(BasePlugin):
    meta = Meta(
        name="orchestrateur_llm",
        priority=-992,      # Juste aprÃ¨s InputPlugin
        version="3.0",
        author="Toi & GPT"
    )

    async def run(self, ctx: Context) -> Context:
        # 1. Chargement des Ã©lÃ©ments mÃ©moire & objectif
        mem         = ctx.get("memoire_contextuelle", "")
        objectif    = ctx.get("objectif", {}).get("but", "rÃ©pondre Ã  une question")
        tache       = ctx.get("tache_courante", "analyse de la demande")
        web_infos   = ctx.get("web_infos", "")
        reflexion   = ctx.get("reflexion_interne", "")
        resultats   = []

        # 2. Extraction message utilisateur prioritaire
        user_msg = ""
        messages = ctx.get("payload", {}).get("messages", [])
        if messages:
            for msg in reversed(messages):
                if msg.get("role") == "user":
                    user_msg = msg.get("content", "").strip()
                    break

        # 3. Fallback si vide
        if not user_msg:
            user_msg = ctx.get("message", "").strip()

        if not user_msg:
            logger.warning("[orchestrateur_llm] Message utilisateur vide â€” injection dâ€™un prompt gÃ©nÃ©rique.")
            user_msg = "(Aucune demande utilisateur explicite dÃ©tectÃ©e.)"

        # 4. Ajout d'autres infos du contexte
        if "calc_result" in ctx:
            resultats.append(f"ğŸ§® RÃ©sultat calculÃ© : {ctx['calc_result']}")
        if "rÃ©sumÃ©" in ctx:
            resultats.append(f"ğŸ“ RÃ©sumÃ© extrait : {ctx['rÃ©sumÃ©']}")
        if "structured_query" in ctx:
            resultats.append(f"ğŸ” RequÃªte structurÃ©e : {ctx['structured_query']}")
        if "infos_extraites" in ctx:
            resultats.append(f"ğŸ“Š DonnÃ©es extraites : {ctx['infos_extraites']}")

        role    = ctx.get("role", "assistant expert")
        ton     = ctx.get("ton", "neutre")
        style   = ctx.get("style", "clair et structurÃ©")

        # 5. Construction finale du prompt
        prompt = f"""Tu es un {role}.
Ton : {ton}
Style : {style}

Objectif global : {objectif}
Ã‰tape actuelle : {tache}

RÃ©flexion interne :
{reflexion or "N/A"}

Historique :
{mem or "Aucune mÃ©moire disponible."}

DonnÃ©es utiles :
{web_infos or "Aucune information web disponible."}
{chr(10).join(resultats)}

Message utilisateur :
{user_msg}

ğŸ§  RÃ©ponds de faÃ§on logique, cohÃ©rente, structurÃ©e et pertinente pour atteindre lâ€™objectif.
"""

        # 6. Injection dans le contexte
        ctx["llm_prompt"] = prompt
        ctx["llm_prompt_locked"] = True
        ctx.setdefault("plugins_log", []).append("OrchestrateurLLM : prompt complet injectÃ©.")
        logger.info("[orchestrateur_llm] Prompt gÃ©nÃ©rÃ© et verrouillÃ©.")

        return ctx
