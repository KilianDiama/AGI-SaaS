# plugins/llm_multiopenrouter.py

import httpx
import os
import logging
from typing import List
from noyau_core import BasePlugin, Context, Meta

logger = logging.getLogger("plugin.llm_multiopenrouter")


class MultiOpenRouterLLMPlugin(BasePlugin):
    meta = Meta(
        name="llm_multiopenrouter",
        priority=105,
        version="1.0",
        author="Matt & GPT"
    )

    async def run(self, ctx: Context) -> Context:
        logger.info("‚úÖ Plugin multi LLM OpenRouter activ√©")

        models: List[str] = ctx.get("llm_models", [])
        backends: List[str] = ctx.get("llm_backends", [])
        message = ctx.get("message_augment√©") or ctx.get("message")

        if not models or not backends or not message:
            logger.warning("‚ö†Ô∏è Donn√©es incompl√®tes (mod√®les ou message manquants)")
            ctx["response"] = "‚ùó Aucun message ou mod√®le valide fourni."
            return ctx

        # V√©rification longueur coh√©rente
        if len(models) != len(backends):
            logger.error("‚ùå Les longueurs de llm_models et llm_backends ne correspondent pas.")
            ctx["response"] = "‚ùå Configuration incorrecte : mod√®les et backends doivent √™tre appari√©s."
            return ctx

        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            ctx["response"] = "‚ùå Cl√© API OpenRouter manquante."
            return ctx

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        responses = []

        for model in models:
            body = {
                "model": model,
                "messages": [{"role": "user", "content": message}]
            }

            try:
                async with httpx.AsyncClient() as client:
                    logger.info(f"üîÅ Appel OpenRouter ‚Üí {model}")
                    resp = await client.post(
                        "https://openrouter.ai/api/v1/chat/completions",
                        headers=headers,
                        json=body,
                        timeout=30.0
                    )
                    resp.raise_for_status()
                    data = resp.json()

                    text = data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
                    logger.info(f"‚úÖ R√©ponse re√ßue de {model}: {text[:100]}")
                    if text:
                        responses.append(f"üîπ *{model}* ‚Üí {text}")
            except Exception as e:
                err_msg = f"‚ùå Erreur avec {model} ‚Üí {str(e)}"
                logger.warning(err_msg)
                ctx.setdefault("errors", []).append({"plugin": "llm_multiopenrouter", "error": err_msg})

        # Fusion simple des r√©ponses
        if responses:
            fusion = "\n\n".join(responses)
            ctx["response"] = f"ü§ñ **Fusion multi-LLM :**\n\n{fusion}"
        else:
            ctx["response"] = "‚ùå Aucune r√©ponse obtenue des mod√®les."

        ctx.setdefault("plugins_log", []).append("MultiOpenRouterLLMPlugin ex√©cut√©")
        return ctx
