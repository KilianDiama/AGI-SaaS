"""
Plugin : evaluation_utilite
R√¥le : √âvaluer la clart√©, la pertinence et l'utilit√© per√ßue de la r√©ponse g√©n√©r√©e
Priorit√© : 5 (apr√®s r√©ponse mais avant apprentissage)
Auteur : AGI_Matt & GPT
"""

from noyau_core import BasePlugin, Context, Meta
import logging

logger = logging.getLogger("plugin.evaluation_utilite")

class EvaluationUtilitePlugin(BasePlugin):
    meta = Meta(
        name="evaluation_utilite",
        priority=5,
        version="1.0",
        author="AGI_Matt & GPT"
    )

    async def run(self, ctx: Context) -> Context:
        plugins_log = ctx.setdefault("plugins_log", [])
        response = ctx.get("llm_response", "")
        objectif = ctx.get("objectif", {}).get("but", "")
        score = 0
        remarques = []

        if not response.strip():
            ctx["evaluation_utilite"] = "‚ùå Pas de r√©ponse √† √©valuer."
            plugins_log.append("EvaluationUtilitePlugin : vide")
            return ctx

        if objectif and any(word in response.lower() for word in objectif.lower().split()):
            score += 1
            remarques.append("üéØ R√©ponse en lien direct avec l‚Äôobjectif.")

        if len(response.strip()) > 100:
            score += 1
            remarques.append("üìè Longueur suffisante pour une explication d√©velopp√©e.")

        if "je ne sais pas" not in response.lower():
            score += 1
            remarques.append("‚úÖ R√©ponse affirm√©e.")

        if "incompr√©hensible" in response.lower() or "je suis confus" in response.lower():
            score -= 1
            remarques.append("‚ö†Ô∏è Clart√© limit√©e d√©tect√©e.")

        conclusion = f"üß† Score d‚Äôutilit√© : {score}/3\n" + "\n".join(remarques)
        ctx["evaluation_utilite"] = conclusion
        plugins_log.append("EvaluationUtilitePlugin : r√©ponse √©valu√©e")
        logger.info(f"[evaluation_utilite] Score : {score}/3")

        return ctx
