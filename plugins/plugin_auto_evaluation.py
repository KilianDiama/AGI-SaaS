""" 
Plugin : auto_evaluation  
R√¥le : G√©n√©rer une auto-√©valuation qualitative de la r√©ponse produite  
Priorit√© : 7 (juste avant apprentissage & m√©moire)  
Auteur : Matthieu & GPT  
"""

import logging
from noyau_core import BasePlugin, Context, Meta

logger = logging.getLogger("plugin.auto_evaluation")

class AutoEvaluationPlugin(BasePlugin):
    meta = Meta(
        name="auto_evaluation",
        priority=7,
        version="1.1",  # mise √† jour version
        author="Matthieu & GPT"
    )

    async def run(self, ctx: Context) -> Context:
        plugins_log = ctx.setdefault("plugins_log", [])
        reponse = ctx.get("response", "")
        objectif = ctx.get("objectif", "Objectif inconnu")

        # S√©curit√© type
        if not isinstance(reponse, str):
            reponse = str(reponse)
        if not isinstance(objectif, str):
            objectif = str(objectif)

        reponse = reponse.strip()

        if not reponse:
            ctx["auto_evaluation"] = "‚ùå √âchec : aucune r√©ponse g√©n√©r√©e."
            plugins_log.append("AutoEvaluationPlugin : aucune r√©ponse √† √©valuer.")
            logger.warning("[auto_evaluation] Pas de contenu pour √©valuation.")
            return ctx

        # Crit√®res simples (pour d√©marrer)
        longueur = len(reponse)
        evals = []

        if longueur < 30:
            evals.append("üî∏ R√©ponse courte : peut manquer de profondeur.")
        elif longueur > 500:
            evals.append("‚úÖ R√©ponse longue : potentiel de profondeur √©lev√©.")
        else:
            evals.append("üü¢ R√©ponse de longueur mod√©r√©e.")

        r_lower = reponse.lower()
        if "je recommande" in r_lower:
            evals.append("üß† Inclut un avis / conseil : bon signe de strat√©gie.")
        if "je ne sais pas" in r_lower:
            evals.append("‚ö†Ô∏è Incertitude d√©tect√©e.")
        if objectif.lower() in r_lower:
            evals.append("üîç L‚Äôobjectif est bien trait√© dans la r√©ponse.")
        else:
            evals.append("‚ùì Lien explicite √† l‚Äôobjectif non trouv√©.")

        conclusion = "Auto-√©valuation :\n" + "\n".join(evals)
        ctx["auto_evaluation"] = conclusion
        ctx["meta_reflexion"] = conclusion  # pour cha√Ænage avec m√©moire

        plugins_log.append("AutoEvaluationPlugin : √©valuation g√©n√©r√©e.")
        logger.info(f"[auto_evaluation] {conclusion.replace(chr(10), ' / ')}")

        return ctx
